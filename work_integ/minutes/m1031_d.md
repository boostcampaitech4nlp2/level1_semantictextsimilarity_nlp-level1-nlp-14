# 10월 31일 오후 팀 회의록
- - -

## 논의 사항 및 멘토링

### Tokenizer max_length
from_pretrained에서 max_length 설정한 것이 반영 안됨   
- Mentoring
    - 모델 마다 tokenizer arguments가 달라서 그럴 수 있음
    - sequence 길이가 작아도 model poisitional embedding 보다 작으면 학습됨
    - padding의 이유는 batch 내의 sequence 길이를 통일시켜주기 위해서
    - max_length를 주면 batch 내 sequence 길이가 줄어드니 computing 비용을 줄일 수 있음
    - batch_size나 다른 hyperparameter를 설정할 때 model 최대 인풋 길이로 배치를 만들었을 때 보다 여유 있게 설정할 수 있음   

-> tokenizer 호출 시에 max_length를 설정하여 batch 내의 sequence 길이를 줄여줌
- max_length 설정 : 175
- 근거 : tokenizer 처리 이후 padding을 붙이지 않은 최대 input_ids 길이 == 171
- 효과 : hidden_size가 보다 큰 모델에 대해서 학습이 가능하게 됨, batch_size 운용 폭이 넓어짐
    - roberta-small hidden_size : 768
    - roberta-large hidden_size : 1024

### Pos tagging & Data preprocessing
- Mentoring
    - tagging 결과물을 받는 모델들이 존재하나 많이 쓰이지는 않음
    - preprocessing 이나 pos-tagging 없이도 성능이 잘 나오는 모델을 만드는 것이 현재 LM 방향성
    - 형태적 분석을 자동화하면 쓸 수 있지만 한국어의 한계로 전처리가 잘 안 먹힌다.  
    - EDA 결과를 모델링에 써먹을 수 있는지를 생각해보고 해야 한다.

### K-fold
- Mentoring
    - parameter 성능 확인용
    - 확인하는 과정이 모델 성능을 좋게 만드는 과정이 됨, hyper parameter 조합들의 성능을 좀 더 뽑아볼 수 있다.
    - validation이 계속 바뀌면서 모든 데이터를 training과 validation에 사용하니까 validation에 치중되지 않는 hyper parameter를 구할 수 있다.
    - early stopping 보다는 k-fold를 활용하는 게 나을 수 있다.

### 방향성
- Mentoring
    - Model selection
        - [SentenceBert](https://arxiv.org/abs/1908.10084)
        - [SimCSE](https://arxiv.org/abs/2104.08821)
    - Model Architecture & Loss function selection
    - Optimizer selection
    - Learning rate selection

### 그 외
- Mentoring
    - mean pooling()
    - 각각의 hidden states 사용, 3-5-8 layer를 많이 사용함, BERT 해석 실험하는 논문이 있다. BERT layer가 초기에는 연관성만, 문법적인 정보, 위로 올라갈 수록 semantic한 정보가 담김. 맨 위의 정보 뿐만 아니라 중간 중간의 정보를 같이 써서 task를 수행
    - Loss : cross-entropy 사용 / regression task로 풀되 0:5를 0:1로 줄여서 sigmoid를 사용한다던가
    - Trainer의 역할 : 원래 전체 코드를 짜도 되는데 학습 코드를 짜는 게 번거로운 일, 그 코드를 덜 짜도 되게 만들어주는 것.
        - 모델 초기화할 때 from_pretrained 사용하는 것 외에 parameter를 원래 method로 초기화하는 것도 있음.

### From now on
- stratified K-fold 시도
- max_length 줄이고 batch_size 늘려서 해보기
- Model selection, 여러 모델 실험해보기